<!DOCTYPE html>
<html>
<head>
<title>README.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="p-style%22text-align-center%22-kubernetes-guide"><p style="text-align: center;">  Kubernetes Guide</h1>
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#--kubernetes-guide">  Kubernetes Guide</a></li>
<li><a href="#table-of-contents">Table of Contents</a>
<ul>
<li><a href="#prerequisites">Prerequisites</a></li>
<li><a href="#optional-packages">Optional packages</a></li>
<li><a href="#getting-started">Getting started</a></li>
<li><a href="#deployment">Deployment</a>
<ul>
<li><a href="#pod-examination-and-configuration">Pod examination and configuration</a></li>
<li><a href="#commands-and-debugging">Commands and Debugging</a></li>
<li><a href="#status-of-different-k8s-components">Status of different K8s components</a></li>
<li><a href="#debugging-pods">Debugging pods</a></li>
</ul>
</li>
<li><a href="#creating-a-custom-configuration-file">Creating a custom configuration file</a>
- <a href="#user-configuration-files-for-crud">User configuration files for CRUD</a>
<ul>
<li><a href="#layers-of-abstraction">Layers of Abstraction</a>
<ul>
<li><a href="#deployment---replicaset---pod---container">Deployment -&gt; ReplicaSet -&gt; Pod -&gt; Container</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#yaml-configuration-file">YAML Configuration File</a>
<ul>
<li><a href="#ports">Ports</a></li>
</ul>
</li>
<li><a href="#complete-application-setup">Complete Application Setup</a>
- <a href="#step-1">Step 1</a>
<ul>
<li><a href="#step-2-create-an-internal-service-so-that-other-pods-can-talk-to-the-mongodb">Step 2: Create an internal service so that other <code>pods</code> can talk to the <code>mongodb</code></a></li>
<li><a href="#step-3-create-mongo-express-extenral-service-along-with-a-configurationmap-file-in-which-well-add-the-database-url">Step 3: Create Mongo Express Extenral Service, along with a ConfigurationMap file, in which we'll add the database URL</a>
<ul>
<li><a href="#apply-the-configmap">Apply the ConfigMap</a></li>
<li><a href="#apply-the-mongo-express">Apply the Mongo-Express</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#welcome-to-mongo-express">Welcome to mongo-express</a>
<ul>
<li><a href="#lets-create-an-external-service-for-the-mongo-express">Let's create an external service for the mongo-express</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#namespaces">Namespaces</a>
<ul>
<li><a href="#what-is-the-need-for-namespaces">What is the need for namespaces?</a></li>
<li><a href="#create-components-in-namespaces">Create Components in Namespaces</a></li>
</ul>
</li>
<li><a href="#k8s-ingress">K8s Ingress</a>
<ul>
<li><a href="#external-service-vs-ingress-configuration-files">External Service vs Ingress Configuration Files</a></li>
</ul>
</li>
<li><a href="#helm-package-manager">Helm Package Manager</a></li>
<li><a href="#kubernetes-volumes">Kubernetes Volumes</a>
<ul>
<li><a href="#persistent-volume">Persistent Volume</a></li>
<li><a href="#persistent-volume-claim">Persistent Volume Claim</a></li>
<li><a href="#storage-class">Storage Class</a></li>
</ul>
</li>
<li><a href="#stateful-and-stateless-applications">Stateful and Stateless applications</a></li>
<li><a href="#k8s-services-overview">K8s Services Overview</a></li>
<li><a href="#testing-between-master-and-worker-in-different-vms">Testing Between Master and Worker in Different VMs</a>
<ul>
<li><a href="#master">Master</a></li>
<li><a href="#worker">Worker</a></li>
</ul>
</li>
<li><a href="#exposure">Exposure</a></li>
<li><a href="#"></a></li>
</ul>
<h3 id="prerequisites">Prerequisites</h3>
<br />
<p><strong>Master &amp; Worker Nodes</strong></p>
<p><code>docker.io</code>-&gt; Container runtime
<br />
<code>kubelet</code> -&gt; Daemon running on systemd. CRUD containers on Pods.
<br />
<code>kubeadm</code> -&gt; Performs the necessary actions to get a minimum viable cluster up and running.
<br />
<code>kubectl</code> -&gt; CLI againts K8s clusters, e.g. deploy applications, inspect and manage cluster resources, and view logs.
<br /></p>
<h3 id="optional-packages">Optional packages</h3>
<br />
<p><code>https transport</code></p>
<p><code>curl</code></p>
<h2 id="getting-started">Getting started</h2>
<p>Creating a cluster with minikube on host machine</p>
<blockquote>
<p>Install minikube to set a local K8s cluster. Not OS-specific.</p>
</blockquote>
<p><code>minikube start</code> <br /></p>
<p><code>kubectl version --output=yaml</code> <br />
<code>kubectl cluster-info</code> <br /></p>
<p>View nodes in the cluster</p>
<p><code>kubectl get nodes</code></p>
<br />
<h2 id="deployment">Deployment</h2>
<p><code>kubectl create deployment &lt;name&gt; --image&lt;image-name-location&gt;</code>
<code>kubectl create deployment &lt;name&gt; --image&lt;image-name-location&gt;</code> <br /></p>
<blockquote>
<p>e.g. =&gt; kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1</p>
</blockquote>
<p>List your deployments:</p>
<p><code>kubectl get deployments</code> <br /></p>
<blockquote>
<p>Pods that are running inside Kubernetes are running on a private,
isolated network. By default they are visible from other pods and
services within the same kubernetes cluster, but not outside that
network. When we use kubectl, we're interacting through an API endpoint
to communicate with or application.</p>
</blockquote>
<p>In another terminal, run:</p>
<pre class="hljs"><code><div>echo -e "\n\n\n\e[92mStarting Proxy. After starting it will not output a response.
Please click the first Terminal Tab\n"; 
kubectl proxy
</div></code></pre>
<p>In yet another terminal, run:</p>
<pre class="hljs"><code><div>curl https://localhost:8001/version
</div></code></pre>
<blockquote>
<p>The API server will automatically create an endpoint for each pod, based
on the pod name, that is also accessible through the proxy. First we
need to get the Pod name, and we'll store in the environment variable
POD_NAME:</p>
</blockquote>
<pre class="hljs"><code><div>export POD_NAME=$(kubectl get pods -o go-template --template
 '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
</div></code></pre>
<p>You can access the Pod through the API by running:</p>
<pre class="hljs"><code><div>curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/ 
</div></code></pre>
<h4 id="pod-examination-and-configuration">Pod examination and configuration</h4>
<p>Pod status can be seen via the describe command of kubectl.</p>
<pre class="hljs"><code><div>kubectl describe pod &lt;pod-name&gt;
</div></code></pre>
<p>Access to the contairer inside the Pod can be granted via the following
command, which iterrates an interactive terminal.</p>
<pre class="hljs"><code><div>kubectl exec -it &lt;pod-name&gt; -- bin/bash
</div></code></pre>
<h4 id="commands-and-debugging">Commands and Debugging</h4>
<pre class="hljs"><code><div>kubectl create deployment [name]
kubectl edit deployment [name]
kubectl delete deployment [name]
</div></code></pre>
<h4 id="status-of-different-k8s-components">Status of different K8s components</h4>
<pre class="hljs"><code><div>kubectl get nodes|pod|services|replicaset|deployment
</div></code></pre>
<h4 id="debugging-pods">Debugging pods</h4>
<pre class="hljs"><code><div>kubectl logs [pod name]
kubectl exec -it [pod-name] -- bin/bash
</div></code></pre>
<h2 id="creating-a-custom-configuration-file">Creating a custom configuration file</h2>
<p>Configuration files in K8s are of .yaml file format. After a Pod,
Container and Deployment are created, a config file can be
created/edited.</p>
<p>Creating a .yaml configuration file:</p>
<pre class="hljs"><code><div>touch nginx-deployment.yaml
nvim nginx-deployment.yaml
</div></code></pre>
<blockquote>
<p>Nginx is a local web server that provides load balancing, allong with HTTP cache and reverse proxy.
<br /></p>
</blockquote>
<blockquote>
<p>From the .yaml configuration file that will be created below, the nginx local server will be deployed inside a containerized environment.</p>
</blockquote>
<p>Inside the .yaml file, a strict and specific syntax must be followed.</p>
<p><strong>Indentation must be strictly followed, otherwise it leads to errors.</strong></p>
<pre class="hljs"><code><div>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:       ##specification for the deployment
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:   ##specification for the pods
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
</div></code></pre>
<h5 id="user-configuration-files-for-crud">User configuration files for CRUD</h5>
<p>After the config file has been created, it can be applied via the following command:</p>
<pre class="hljs"><code><div>kubectl apply -f nginx-deployment.yaml
</div></code></pre>
<p>If we run the following command, we can see the new deployment is ready
and running:</p>
<pre class="hljs"><code><div>kubectl get pod
</div></code></pre>
<p>Similarly for the deployment:</p>
<pre class="hljs"><code><div>kubectl get deployment
</div></code></pre>
<h3 id="layers-of-abstraction">Layers of Abstraction</h3>
<br />
<h5 id="deployment---replicaset---pod---container">Deployment -&gt; ReplicaSet -&gt; Pod -&gt; Container</h5>
<h2 id="yaml-configuration-file">YAML Configuration File</h2>
<p><strong>Strict Syntax Indentation!!</strong></p>
<br />
<blockquote>
<p>For autogenerating config files, K8s gets the status from the etch, which hold the current status of any K8s component!</p>
</blockquote>
<br />
<p>The basic idea is that inside a <code>.yaml</code> configuration file exist other configuration files as <code>metadata</code> and <code>spec</code> sections.</p>
<p><code>Pods</code> should have their own configuration inside of the <code>Deployments</code> configuration file.
All <code>Pods</code> will be defined.</p>
<p>Inside the <code>metadata</code> of each <code>pod</code>, exist the <code>spec</code> section.
The <code>spec</code> section covers the name of the <code>container</code>, the <code>image</code> running inside the <code>container</code>, along with the <code>containerPort</code> inside the private network.</p>
<p>The connection between <code>Services</code> and <code>Deployments</code> is established with <code>Labels</code> and <code>Selectors</code>.</p>
<p>Specificlly, the <code>metadata</code> part contains the <code>labels</code>, and the <code>spec</code> part contains <code>Selectors</code>.</p>
<p>This way, the <code>Deployment</code> knows with <code>Pods</code> belong to specific applications.</p>
<p>The <code>Deployment</code> has its own label, which will be used by the <code>Service</code> selector which makes a connection between the <code>Service</code> and the <code>Deployment</code>.</p>
<h4 id="ports">Ports</h4>
<p>Both <code>Service</code> and <code>Deployment</code> need to have <code>Ports</code> defined.
That way, the DB Service knows with which port to communicate with the nginx Service, and to which <code>Pod</code> it
should forward the request, but also which <code>Pods</code> are listening.</p>
<p><strong>Some examples:</strong></p>
<table>
<tr>
<th>nginx-deployment.yaml</th>
<th>nginx-service.yaml</th>
</tr>
<tr>
<td>
<pre>
  apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.16
        ports:
        - containerPort: 8080
</pre>
</td>
<td>
<pre class="hljs"><code><div>apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: <span class="hljs-number">80</span>
      targetPort: <span class="hljs-number">8080</span>
 
</div></code></pre>
</td>
</tr>
</table>
<p>After the configuration files are created, we can apply them to both deployment and service.</p>
<p><code>kubectl apply -f nginx-deployment.yaml</code></p>
<p><code>kubectl apply -f nginx-service.yaml</code></p>
<p>Now we can see that 2 replicas are running, as it was defined in the config file of the <code>nginx-deployment.yaml</code></p>
<p><code>kubectl get pod</code></p>
<p>And the service we created from the <code>nginx-service.yaml</code> is up and running.</p>
<p><code>kubectl get service</code></p>
<hr>
<p>Now we can get the information of the auto-generated config file of the <code>nginx-service</code> by running:</p>
<p><code>kubectl describe service nginx-service</code></p>
<p>Inside which description, we can find the <code>Endpoints</code>, which describe the IP-Addresses of the Pods, along with the port they're listening.</p>
<p>We can check if the ports of the <code>Pods</code> are correct by running:</p>
<p><code>kubectl get pod -o wide</code></p>
<hr>
<p>Finally, let's check the status, in .yaml format, that K8s automatically generates, and save it in a file:
The status info resides in the <code>etcd</code>, which stores the the status of the whole cluster, including every component.</p>
<p><code>kubectl get deployment nginx-deployment -o yaml &gt; nginx-deployment-result.yaml</code></p>
<blockquote></blockquote>
<tr>
<th>nginx-deployment-result.yaml</th>
</tr>
<tr>
<td>
<pre class="hljs"><code><div>apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: <span class="hljs-string">"1"</span>
    kubectl.kubernetes.io/last-applied-configuration: |
      {<span class="hljs-attr">"apiVersion"</span>:<span class="hljs-string">"apps/v1"</span>,<span class="hljs-attr">"kind"</span>:<span class="hljs-string">"Deployment"</span>,<span class="hljs-attr">"metadata"</span>:{<span class="hljs-attr">"annotations"</span>:{},<span class="hljs-attr">"labels"</span>:{<span class="hljs-attr">"app"</span>:<span class="hljs-string">"nginx"</span>},<span class="hljs-attr">"name"</span>:<span class="hljs-string">"nginx-deployment"</span>,<span class="hljs-attr">"namespace"</span>:<span class="hljs-string">"default"</span>},<span class="hljs-attr">"spec"</span>:{<span class="hljs-attr">"replicas"</span>:<span class="hljs-number">2</span>,<span class="hljs-attr">"selector"</span>:{<span class="hljs-attr">"matchLabels"</span>:{<span class="hljs-attr">"app"</span>:<span class="hljs-string">"nginx"</span>}},<span class="hljs-attr">"template"</span>:{<span class="hljs-attr">"metadata"</span>:{<span class="hljs-attr">"labels"</span>:{<span class="hljs-attr">"app"</span>:<span class="hljs-string">"nginx"</span>}},<span class="hljs-attr">"spec"</span>:{<span class="hljs-attr">"containers"</span>:[{<span class="hljs-attr">"image"</span>:<span class="hljs-string">"nginx:1.16"</span>,<span class="hljs-attr">"name"</span>:<span class="hljs-string">"nginx"</span>,<span class="hljs-attr">"ports"</span>:[{<span class="hljs-attr">"containerPort"</span>:<span class="hljs-number">8080</span>}]}]}}}}
  creationTimestamp: <span class="hljs-string">"2023-03-23T10:54:56Z"</span>
  generation: <span class="hljs-number">1</span>
  labels:
    app: nginx
  name: nginx-deployment
  namespace: default
  resourceVersion: <span class="hljs-string">"96574"</span>
  selfLink: /apis/apps/v1/namespaces/default/deployments/nginx-deployment
  uid: e1075fa3<span class="hljs-number">-6468</span><span class="hljs-number">-43</span>d0<span class="hljs-number">-83</span>c0<span class="hljs-number">-63</span>fede0dae51
spec:
  progressDeadlineSeconds: <span class="hljs-number">600</span>
  replicas: <span class="hljs-number">2</span>
  revisionHistoryLimit: <span class="hljs-number">10</span>
  selector:
    matchLabels:
      app: nginx
  strategy:
    rollingUpdate:
      maxSurge: <span class="hljs-number">25</span>%
      maxUnavailable: <span class="hljs-number">25</span>%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: <span class="hljs-literal">null</span>
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:<span class="hljs-number">1.16</span>
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: <span class="hljs-number">8080</span>
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: <span class="hljs-number">30</span>
status:
  availableReplicas: <span class="hljs-number">2</span>
  conditions:
  - lastTransitionTime: <span class="hljs-string">"2023-03-23T10:54:59Z"</span>
    lastUpdateTime: <span class="hljs-string">"2023-03-23T10:54:59Z"</span>
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: <span class="hljs-string">"True"</span>
    type: Available
  - lastTransitionTime: <span class="hljs-string">"2023-03-23T10:54:56Z"</span>
    lastUpdateTime: <span class="hljs-string">"2023-03-23T10:54:59Z"</span>
    message: ReplicaSet <span class="hljs-string">"nginx-deployment-7d64f4b574"</span> has successfully progressed.
    reason: NewReplicaSetAvailable
    status: <span class="hljs-string">"True"</span>
    type: Progressing
  observedGeneration: <span class="hljs-number">1</span>
  readyReplicas: <span class="hljs-number">2</span>
  replicas: <span class="hljs-number">2</span>
  updatedReplicas: <span class="hljs-number">2</span>
</div></code></pre>
<p><strong>We can delete the <code>Deployment</code> and <code>Service</code> by deleting the configuration files.</strong></p>
<p><code>kubectl delete -f nginx-deployment.yaml</code>
<code>kubectl delete -f nginx-service.yaml</code></p>
<br />
<h2 id="complete-application-setup">Complete Application Setup</h2>
<p>Implementation of simple web application using <code>mongo-express</code> &amp; <code>mongoDB</code>.</p>
<p>First, we're going to create a <code>mongoDB</code> <code>Pod</code>, and to talk to the <code>Pod</code> we're going to need a service.
We're going to create an Internal Service, meaning that no external requests are allowed to the pod,
only components in the same cluster are able to talk to it.</p>
<p>Then we'll create a <code>mongo-express</code> <code>Deployment</code></p>
<p>We'll create a <code>Deployment.yaml</code> for the <code>mongo-express</code> deployment, which will be provided with
environmental variables, that will allow it to connect to the <code>mongoDB</code>.</p>
<p>The <code>mongoDB</code> will consist of the following:</p>
<ul>
<li>ConfigMap -&gt; DB URL</li>
<li>Secret    -&gt; DB User, DB Pwd</li>
</ul>
<p>So the Request Flow will look like the following:</p>
<blockquote>
<p>The request comes from the browser, it goes through the <code>mongo-express external Service</code>, which will forward
it to the <code>mongo-express</code> <code>Pod</code>.
The <code>Pod</code> then will connect to the <code>mongoDB</code> Internal Service, which will forward it to the <code>mongoDB</code> <code>Pod</code>,
where it will authenticate the request by using the credentials of the <code>Secret</code> module of the <code>mongoDB</code>.</p>
</blockquote>
<ul>
<li>
<p>Start <code>minikube</code> if using a local cluster instance in your host machine.</p>
</li>
<li>
<p>Run <code>kubectl get all</code> to view all the components inside the cluster.</p>
</li>
</ul>
<h5 id="step-1">Step 1</h5>
<p>Create the <code>mongoDB</code> Deployment.</p>
<table>
<tr>
<th>mongo.yaml</th>
<th>mongodb-secret.yaml</th>
</tr>
<tr>
<td>
<pre>
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: mongodb-service
  spec:
    selector:
      app: mongodb
    ports:
      - protocol: TCP
        port: 27017
        targetPort: 27017
</pre>
</td>
<td>
<pre class="hljs"><code><div>apiVersion: v1
kind: Secret
metadata:
    name: mongodb-secret
type: Opaque
data:
    mongo-root-username: dXNlcm5hbWU=    #generated as base64 value with command 
    mongo-root-password: cGFzc3dvcmQ=    # `echo -n 'username/password' | base64

</div></code></pre>
</td>
</tr>
</table>
<br />
<p>Then we can apply the secret with <code>kubectl</code></p>
<p><code>kubectl apply -f mongodb-secret.yaml</code></p>
<p>Check the <code>secret</code> status:</p>
<p><code>kubectl get secret</code></p>
<p>Create the deployment</p>
<p><code>kubectl apply -f mongo.yaml</code></p>
<p>Check the pod status</p>
<p><code>kubectl get pod</code></p>
<blockquote>
<p>If it takes a bit for the <code>pod</code> to be created, you can run <code>kubectl get pod --watch</code> to have live feedback.</p>
</blockquote>
<br />
### Step 2: Create an internal service so that other `pods` can talk to the `mongodb`
<p>See ending section of file <code>mongo.yaml</code> in Step 1.</p>
<br />
<h3 id="step-3-create-mongo-express-extenral-service-along-with-a-configurationmap-file-in-which-well-add-the-database-url">Step 3: Create Mongo Express Extenral Service, along with a ConfigurationMap file, in which we'll add the database URL</h3>
<br />
<table>
<tr>
<th>mongo-express.yaml</th>
</tr>
<tr>
<td>
<pre>
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: mongodb-service
  spec:
    selector:
      app: mongodb
    ports:
      - protocol: TCP
        port: 27017
        targetPort: 27017
</pre>
</td>
</tr>
</table>
<br />
<h4 id="apply-the-configmap">Apply the ConfigMap</h4>
<p><code>kubectl apply -f mongo-configmap.yaml</code></p>
<h4 id="apply-the-mongo-express">Apply the Mongo-Express</h4>
<p><code>kubectl apply -f mongo-express.yaml</code></p>
<p>We can see the logs for further information and confirmation that everything is going smoothly:</p>
<p><code>kubectl logs mongo-express-5bf4b56f47-5n9vq</code></p>
<blockquote>
<p>change the name of the mongo-express with the name if the instance in your machine.</p>
</blockquote>
<table>
<tr>
<th>Mongo-express logs</th>
<th></th>
</tr>
<tr>
<td>
<pre>
<h2 id="welcome-to-mongo-express">Welcome to mongo-express</h2>
<p>(node:7) [MONGODB DRIVER] Warning: Current Server Discovery and Monitoring engine is deprecated, and will be removed in a future version. To use the new Server Discover and Monitoring engine, pass option { useUnifiedTopology: true } to the MongoClient constructor.
Mongo Express server listening at <a href="http://0.0.0.0:8081">http://0.0.0.0:8081</a>
Server is open to allow connections from anyone (0.0.0.0)
basicAuth credentials are &quot;admin:pass&quot;, it is recommended you change this in your config.js!</p>
</pre>
</td>
</tr>
</table>
<br />
<p>Now that everything is running correctly, the last step is to create an external service
so that we can access the mongo-express from a browser.</p>
<h4 id="lets-create-an-external-service-for-the-mongo-express">Let's create an external service for the mongo-express</h4>
<br />
<table>
<tr>
<th>mongo-express.yaml</th>
</tr>
<tr>
<td>
<pre>
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-express
  labels:
    app: mongo-express
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-express
  template:
    metadata:
      labels:
        app: mongo-express
    spec:
      containers:
      - name: mongo-express
        image: mongo-express
        ports:
        - containerPort: 8081
        env:
        - name: ME_CONFIG_MONGODB_ADMINUSERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: ME_CONFIG_MONGODB_ADMINPASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password
        - name: ME_CONFIG_MONGODB_SERVER
          valueFrom:
            configMapKeyRef:
              name: mongodb-configmap
              key: database_url
---
apiVersion: v1
kind: Service
metadata:
  name: mongo-express-service
spec:
  selector:
    app: mongo-express
  type: LoadBalancer  
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
      nodePort: 3000>
</pre>
</td>
</tr>
</table>
<br />
<p>Now, running the command:</p>
<p><code>kubectl get service</code></p>
<p>will give us all the information we need for the service that was created.
Most importantly, the Cluster-IP address, along with port and the type of the service.</p>
<p>The External-IP address is not yet specified, so we need to assign to an external service a public IP-Adress.</p>
<p><code>minikube service mongo-express-service</code></p>
<p>And as a result, a browser will open automatically to the Mongo Express page.</p>
<h1 id="namespaces">Namespaces</h1>
<p>Namespaces can be used to organize resources in a cluster.</p>
<p>A cluster can have multiple Namespaces. It helps to think of Namespaces as virtual
cluster inside a Kubernetes cluster.</p>
<p>Upon cluster creation, K8s gives us 4 default Namespaces.</p>
<p>The following command can be run to view said namespaces:</p>
<p><code>kubectl get namespaces</code></p>
<table>
<thead>
<tr>
<th>Namespaces</th>
<th>Function</th>
</tr>
</thead>
<tbody>
<tr>
<td>defualt</td>
<td>Resources you create are located here.</td>
</tr>
<tr>
<td>kube-node-lease</td>
<td>Holds info on the heartbeats of nodes. Determines the availability of a node</td>
</tr>
<tr>
<td>kube-public</td>
<td>Contains the publicly-accesible data.</td>
</tr>
<tr>
<td>kube-system</td>
<td>Not to be altered, contains system processes</td>
</tr>
<tr>
<td>kubernetes-dashboard</td>
<td>Minikube-Specific</td>
</tr>
</tbody>
</table>
<p>To create a new namespace, use the following command or via a config file:</p>
<p><code>kubectl create namespace my-namespace</code></p>
<h3 id="what-is-the-need-for-namespaces">What is the need for namespaces?</h3>
<p>Can compartmentalize all components of a cluster for easier user.
Especially if there are multiple instances of Deployments, Pods, Services and configmaps inside a Cluster.</p>
<p>E.g. There can be different namespaces for Monitoring tools, Database, etc.</p>
<p>Also important if multiple teams use the same Deployment.
What this offers, is that all teams can use the same deployment, inside the same cluster,
but use a different Namespace, as not to disrupt each other.</p>
<p>Another use case is Resource Sharing: Staging and Development.
That way you can deploy in one cluster, and every namespace has access to the resources of the deployment.</p>
<h3 id="create-components-in-namespaces">Create Components in Namespaces</h3>
<p>Let's create a <code>configmap</code> file inside a specific Namespace.</p>
<table>
<tr>
<th>mysql-configmap.yaml</th>
</tr>
<tr>
<td>
<pre>
 apiVersion: v1
 kind: ConfigMap
 metadata:
  name: mysql-configmap
 data:
  db_url: mysql-service.database
</pre>
</td>
</tr>
</table>
<p><strong>If no namespace flag is given, the configmap will be created in the default namespace</strong></p>
<p><code>kubectl apply -f mysql-configmap.yam --namespace=my-namespace</code></p>
<p>Alternatively, the namespace can be specified inside the config file.</p>
<table>
<tr>
<th>mysql-configmap.yaml</th>
</tr>
<tr>
<td>
<pre>
 apiVersion: v1
 kind: ConfigMap
 metadata:
  name: mysql-configmap
  namespace: my-namespace
 data:
  db_url: mysql-service.database
</pre>
</td>
</tr>
</table>
<p>To see the configmap inside the custom namespace:</p>
<p><code>kubectl get configmap -n my-namespace</code></p>
<p><strong>Also check <code>kubens</code> for changing the default namespace</strong></p>
<h1 id="k8s-ingress">K8s Ingress</h1>
<p>Ingress replaces the external service of an application, most likely in production,
so that the user can navigate to the service via brower with a <code>https</code> <code>domain name</code>
instead of th <code>IP Address</code> and the specific <code>Port</code> of the application.</p>
<h2 id="external-service-vs-ingress-configuration-files">External Service vs Ingress Configuration Files</h2>
<table>
<tr>
<table>
<tr>
<th>external-service.yaml</th>
<th>ingress.yaml</th>
</tr>
<tr>
<td>
<pre>
 apiVersion: v1
 kind: Service
 metadata:
  name: myapp-external-service
 spec:
  selector:
    app: myapp
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
      nodePort: 35010
</pre>
</td>
<td>
<pre class="hljs"><code><div>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
spec:
  rules:
  - host: dashboard.com
    http:
      paths:
      - path: /
        pathType: Exact  
        backend:
          service:
            name: kubernetes-dashboard
            port: 
              number: 80

</div></code></pre>
</td>
</tr>
</table>
<p>After the application of the configuration files have been executed with:</p>
<p><code>kubectl aply-f dashboard-ingress.yaml</code></p>
<p>Run the following command to get the status of the ingress:</p>
<p><code>kubectl get ingress -n kubernetes-dashboard</code></p>
<p>Now we can see the IP-Address of the dashboard we created along with the domain name.</p>
<table>
<thead>
<tr>
<th>NAME</th>
<th>CLASS</th>
<th>HOSTS</th>
<th>ADDRESS</th>
<th>PORTS</th>
<th>AGE</th>
</tr>
</thead>
<tbody>
<tr>
<td>dashboard-ingress</td>
<td><none></td>
<td>dashboard.com</td>
<td>192.168.49.2</td>
<td>80</td>
<td>23m</td>
</tr>
</tbody>
</table>
<p>Next, we need to add the IP Address and the domain name to the /etc/hosts file of the machine/cluster.</p>
<p><code>sudo -- sh -c &quot;echo '192.168.49.2 dashboard.com' &gt;&gt; /etc/hosts&quot;</code></p>
<p>Optional: There may be need for the command <code>kubectl proxy</code>to be run in another terminal.</p>
<p>Now we can access the domain <code>dashboard.com</code></p>
<p>Finally, a token needs to be generated via the command :</p>
<p><code>kubectl -n kubernetes-dashboard create token admin-user</code></p>
<p>And we have been granted access to the dashboard:</p>
<p><img src="dashboard.png" alt="K8s Dashboard running in minikube cluster"></p>
<h1 id="helm-package-manager">Helm Package Manager</h1>
<p>Helm Package Manager is a user repository that allows user to user already implemented
deployment containers that can be added to specific deployments via <code>.yaml</code> configuration files.</p>
<p>Package repositories, known as Helm Charts, can be accessed via <code>CLI</code> of <code>Helm Hum</code>.</p>
<p><code>helm search &lt;keyword&gt;</code></p>
<p>Helm can also be used as a <code>Templating Eninge</code>.</p>
<p>Specifically in cases that there exists multiple microservices of the same nature, but different versions,
a common blueprint can be defined and also the dynamic values are replaced by placeholders.</p>
<p>In essense, Helm helps with version control, along with deployment rollout services.</p>
<h1 id="kubernetes-volumes">Kubernetes Volumes</h1>
<p>This section covers the topic of persisting data in Kubernetes using volums.</p>
<p>There are 3 components of Kubernetes storage:</p>
<ol>
<li>Persistent Volume</li>
<li>Persistent Volume Claim</li>
<li>Storage Class</li>
</ol>
<p>Storage Requirements:</p>
<ul>
<li>Storage that doesn't depend on the pod lifecycle.</li>
<li>Storage must be available on all nodes.</li>
<li>Storage needs to survive even if cluster crashes.</li>
</ul>
<h3 id="persistent-volume">Persistent Volume</h3>
<p>Persistent Volume is a cluster resource, similar to the CPU or RAM that is used to store data.
Just like any other K8s component, it is created via <code>.yaml</code> configuration file, on which
the kind of the component is specified, along with the storage capacity and access permission.</p>
<p>Each Cluster can have multiple storage options (local/nfs or cloud) configured for simultaneously usage.</p>
<h3 id="persistent-volume-claim">Persistent Volume Claim</h3>
<p>Persistent Volume Claim (pvc) is configured also via a <code>.yaml</code> configuration file that specifies
the storage volume type and capacity it wants to claim, along with other criteria.</p>
<p><strong><u>Note </u>:</strong> PVCs must be in the same namespace as the Pod using the claim!</p>
<h3 id="storage-class">Storage Class</h3>
<p>A StorageClass provides a way for administrators to describe the &quot;classes&quot; of storage they offer.</p>
<p>Example <code>.yaml</code></p>
<pre class="hljs"><code><div>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Retain
allowVolumeExpansion: true
mountOptions:
  - debug
volumeBindingMode: Immediate
</div></code></pre>
<h1 id="stateful-and-stateless-applications">Stateful and Stateless applications</h1>
<p>In Kubernetes, applications can be classified as either stateful or stateless. The classification depends on how an application manages its state, that is,
the data that it needs to store and retrieve over time.</p>
<p>Stateless applications do not require persistent storage of data, meaning that the application does not need to store any information between requests.
They are designed to be easily replicated and scaled horizontally, which means that multiple instances of the application can be running at the same time,
and requests can be load balanced across them. Stateless applications are often used for web servers, load balancers, or microservices that perform a specific function.</p>
<p>Stateful applications, on the other hand, require persistent storage of data, meaning that the application needs to store and retrieve data over time.
They are typically more complex to manage than stateless applications because they have data dependencies and require data to be maintained across instances.
Examples of stateful applications include databases, key-value stores, and file systems.</p>
<p>When deploying stateful applications in Kubernetes, it is important to consider the storage requirements and how data will be managed across instances.
Kubernetes provides features such as StatefulSets, which allow you to manage stateful applications and ensure that each instance is uniquely identifiable
and has persistent storage. This makes it easier to scale and manage stateful applications in a Kubernetes environment.</p>
<h1 id="k8s-services-overview">K8s Services Overview</h1>
<p>Kubernetes (k8s) is a container orchestration platform that allows you to manage, scale, and deploy containerized applications. One of the key features of
Kubernetes is the ability to define and manage services, which provide network connectivity to groups of pods running your application.</p>
<p>Here is an overview of Kubernetes services:</p>
<ul>
<li>ClusterIP: This is the default service type in Kubernetes. It provides a virtual IP address that can be used to access pods within the same cluster.</li>
<li>ClusterIP services are only accessible from within the cluster.</li>
<li>NodePort: This service type exposes a port on every node in the cluster, and routes traffic to the associated pod. NodePort services are accessible
from outside the cluster by connecting to the node's IP address and the specified port.</li>
<li>LoadBalancer: This service type provisions a load balancer in the cloud provider's infrastructure and directs traffic to the associated pod. LoadBalancer
services are accessible from outside the cluster through the load balancer's IP address.</li>
<li>ExternalName: This service type maps a service name to an external DNS name, allowing pods within the cluster to access an external service without exposing
the external name to the pod.</li>
</ul>
<p>Overall, Kubernetes services allow you to easily expose your application to the outside world and manage the network connectivity between your application's components.</p>
<hr>
<h1 id="testing-between-master-and-worker-in-different-vms">Testing Between Master and Worker in Different VMs</h1>
<ul>
<li><input type="checkbox" id="checkbox0" checked="true"><label for="checkbox0">Communication between Master-Node</label></li>
<li><input type="checkbox" id="checkbox1" checked="true"><label for="checkbox1">Document commands</label></li>
<li><input type="checkbox" id="checkbox2"><label for="checkbox2">Experiment with deployments</label></li>
<li><input type="checkbox" id="checkbox3"><label for="checkbox3">Examine image deployment in pods</label></li>
</ul>
<blockquote>
<p>Note that both Master and Worker are running on different VMs inside the same host machine.</p>
</blockquote>
<blockquote>
<p>We suppose that the prerequisite packages are already installed as per the K8s documentation:</p>
<ul>
<li>kubectl</li>
<li>kubeadmin</li>
<li>kubelet</li>
<li>docker, or other CRI</li>
</ul>
<p>Also make sure that all the above packages' service are up and running.</p>
</blockquote>
<h3 id="master">Master</h3>
<ul>
<li>
<p>Run the <code>ip addr</code> command and make a note of the Master node's IP adress.</p>
<ul>
<li>
<pre class="hljs"><code><div>enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP  group default qlen 1000
link/ether 52:54:00:8a:38:61 brd ff:ff:ff:ff:ff:ff
inet 192.168.122.222/24 brd 192.168.122.255 scope global dynamic noprefixroute enp1s0
  valid_lft 2978sec preferred_lft 2978sec
inet6 fe80::5fbf:1833:73da:d07c/64 scope link noprefixroute 
  valid_lft forever preferred_lft forever
</div></code></pre>
</li>
</ul>
</li>
<li>
<p>Login as <code>root</code> on the shell promt: <code>sudo su -</code></p>
</li>
<li>
<p>Initialize the cluster on the Master node:</p>
<ul>
<li><code>kubeadm init --pod-network-cidr=10.10.0.0/16 --apiserver-advertise-address=192.168.122.222</code></li>
</ul>
</li>
<li>
<p>The above command brings the following message to the promt:</p>
<ul>
<li>
<pre class="hljs"><code><div>
To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.222:6443 --token maydxf.wdebk75ipe0e3j5q \
  --discovery-token-ca-cert-hash sha256:1a7ce87c65adf9ea047bb8d093b98e72f8aefc17d632e6003dbe3be6d728dd4d 
</div></code></pre>
</li>
</ul>
</li>
<li>
<p>Then, as a regular user we run the following as shown in the above message:</p>
<ul>
<li>
<pre class="hljs"><code><div>  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
</div></code></pre>
</li>
</ul>
</li>
<li>
<p>And then we need to deploy o pod network to the cluster. In this case, the K8s addon used is called <code>Calico</code></p>
<ul>
<li>
<pre class="hljs"><code><div>kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/tigera-operator.yaml
</div></code></pre>
</li>
<li>
<pre class="hljs"><code><div>curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/custom-resources.yaml -O
</div></code></pre>
</li>
<li>
<pre class="hljs"><code><div>kubectl create -f custom-resources.yaml
</div></code></pre>
</li>
<li>
<pre class="hljs"><code><div>curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O
</div></code></pre>
</li>
<li>
<pre class="hljs"><code><div>kubectl apply -f calico.yaml
</div></code></pre>
</li>
</ul>
<hr>
<h3 id="worker">Worker</h3>
</li>
<li>
<p>Now, we need to order the worker node to join the Master node's network:</p>
<blockquote>
<p>In the message shown after the cluster initialization we can see an auto-generated command that K8s gives us to connect any pod to the cluster network.</p>
</blockquote>
<ul>
<li>
<pre class="hljs"><code><div> kubeadm join 192.168.122.222:6443 --token uld3oe.goqqlexe51z135og \
   --discovery-token-ca-cert-hash sha256:c889f1c891f541d55d75b8e04c79bd67cda18557e2fe4f6accfb10675e3ef45b 
 
</div></code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Finally the following message is being shown on the worker prompt:</strong></p>
<ul>
<li>
<pre class="hljs"><code><div>This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

</div></code></pre>
</li>
</ul>
</li>
<li>
<p>And we can see the Worker node has been connected to the cluster network from the Master's promt.</p>
</li>
</ul>
<pre class="hljs"><code><div>    master@master:~/Desktop$ kubectl get node
     
    NAME         STATUS   ROLES           AGE     VERSION
    master       Ready    control-plane   10m     v1.26.3
    slave-node   Ready    &lt;none&gt;          9m21s   v1.26.3
</div></code></pre>
<h1 id="exposure">Exposure</h1>
<p>Note to self:  Read the following <a href="https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/">https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/</a></p>
<h1 id=""></h1>

</body>
</html>
